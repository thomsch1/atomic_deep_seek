# How to Run the Deep Search AI Application

## **Quick Start (Development Mode)**

### **1. Prerequisites**
- **Python 3.11+** and **Node.js 18+**
- **Docker & Docker Compose** (for production)
- **Google Gemini API Key** ([Get one here](https://makersuite.google.com/app/apikey))

### **2. Setup Environment**
```bash
# Navigate to project
cd /mnt/c/Users/procy/projects_overall/deep_search/real_deep_search/real_deep_ai_dev

# Setup backend environment (API key already configured)
cd backend
# .env file already exists with GEMINI_API_KEY

# Install backend dependencies
pip install .

# Install frontend dependencies  
cd ../frontend
npm install
```

### **3. Start Development Servers**
```bash
# From project root - starts both frontend and backend
make dev
```
**OR start separately:**
```bash
# Terminal 1: Backend
cd backend && langgraph dev

# Terminal 2: Frontend  
cd frontend && npm run dev
```

### **4. Access Application**
- **Frontend UI**: http://localhost:5173
- **LangGraph UI**: http://localhost:2024
- **Backend API**: http://localhost:2024

---

## **Production Mode (Docker)**

### **1. Build and Run**
```bash
# Set environment variables and start
GEMINI_API_KEY=your_api_key docker-compose up -d
```

### **2. Access Production App**
- **Application**: http://localhost:8123/app
- **API**: http://localhost:8123

---

## **Test the Application**

### **CLI Test (Quick Verification)**
```bash
cd backend
python examples/cli_research.py "What are the latest trends in renewable energy?"
```

### **Web Interface Test**
1. Open http://localhost:5173
2. Enter a research question: *"Who won the Euro 2024 and scored the most goals?"*
3. Select effort level (Low/Medium/High) and model
4. Click "Search" and watch the real-time research process

---

## **Application Features**

**Research Process You'll See:**
1. **üéØ Generating Search Queries** - AI creates optimized search terms
2. **üîç Web Research** - Parallel web searches gather sources  
3. **ü§î Reflection** - AI analyzes results for gaps
4. **‚úÖ Finalizing Answer** - Synthesizes complete response with citations

**Effort Levels:**
- **Low**: 1 query, 1 research loop (fast)
- **Medium**: 3 queries, 3 loops (balanced)  
- **High**: 5 queries, 10 loops (comprehensive)

**AI Models:**
- **Gemini 2.0 Flash**: Fastest responses
- **Gemini 2.5 Flash**: Balanced speed/quality
- **Gemini 2.5 Pro**: Highest quality

---

## **Troubleshooting**

### **Port Conflicts**
```bash
# Kill processes on ports
kill -9 $(lsof -t -i:2024)  # Backend
kill -9 $(lsof -t -i:5173)  # Frontend
```

### **API Key Issues**
- Verify your Gemini API key is set in `backend/.env`
- Test connectivity: Visit LangGraph UI at http://localhost:2024

### **Dependencies**
```bash
# Reinstall if needed
cd backend && pip install --force-reinstall .
cd frontend && rm -rf node_modules && npm install
```

The application is ready to run with comprehensive AI-powered research capabilities, real-time streaming, and an intuitive interface!

## üß™ Testing the Application

### Backend Tests
```bash
cd backend
pip install -e .
pip install pytest pytest-asyncio pytest-mock

# Run validation tests
python3 -m pytest tests/test_simple_validation.py -v
python3 -m pytest tests/test_simple_functional.py -v

# Run all working tests
python3 -m pytest tests/test_simple_validation.py tests/test_simple_functional.py -v
```

### Frontend Tests  
```bash
cd frontend
npm install
npm test                    # Run all tests
npm run test:coverage      # Run with coverage report
```

### Test Results Summary
- ‚úÖ **Backend**: 17/17 tests passing
- ‚úÖ **Frontend**: 13/13 tests passing  
- ‚úÖ **API Validation**: Health endpoints working
- ‚úÖ **Error Handling**: Robust error management
- ‚úÖ **Performance**: Sub-second test execution

See `TEST_RESULTS.md` for detailed test analysis and coverage information.